{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n",
    "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # train set features\n",
    "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # train set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n",
    "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # test set features\n",
    "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # test set labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all the classes\n",
    "classes = np.array(test_dataset[\"list_classes\"][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the train and test set labels\n",
    "train_set_y = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "test_set_y = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 50\n",
    "plt.imshow(train_set_x_orig[index])\n",
    "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape of datasets\n",
    "\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "\n",
    "print (\"train_set_x_orig shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_orig shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing dataset\n",
    "train_set_x = train_set_x_flatten/255\n",
    "test_set_x = test_set_x_flatten/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1+np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU function\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model parameters W and b\n",
    "def init_params(layer_dims):\n",
    "    np.random.seed(3)\n",
    "    params = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        params['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])/ np.sqrt(layer_dims[l-1])\n",
    "        params['b'+str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation step (for 1 layer)\n",
    "def forward_prop_step(A_prev, W, b, activ_func):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    \n",
    "    if activ_func == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activ_func == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "\n",
    "    cache = (A_prev, W, b, Z)    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation - full iteration (for 1 to L layers)\n",
    "def forward_prop_model(X, params): \n",
    "    caches=[]\n",
    "    L = len(params)//2\n",
    "    A = X\n",
    "    \n",
    "    # Apply ReLU activaition function, while propagating forward over the layers from 1 to L-1\n",
    "    for l in range (1,L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_prop_step(A_prev, params['W'+str(l)], params['b'+str(l)], 'relu')\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Apply Sigmoid activaition function, while propagating forward over the last layer (layer L)\n",
    "    AL, cache = forward_prop_step(A, params['W'+str(L)], params['b'+str(L)], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def cost_func(A, Y):\n",
    "    m = Y.shape[1]    \n",
    "    cost = -1/m * np.sum(np.multiply(Y, np.log(A)) + np.multiply(1-Y, np.log(1-A)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating dZ for backpropagation step for the layer with sigmoid\n",
    "def sigmoid_backward(dA, Z):\n",
    "    g_deriv = np.multiply (sigmoid(Z), (1-sigmoid(Z)))\n",
    "    dZ = np.multiply (dA, g_deriv)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating dZ for backpropagation step for the layer with ReLU\n",
    "def relu_backward(dA, Z):\n",
    "    g_deriv = np.where(Z<0, 0, 1)    \n",
    "    dZ = np.multiply(dA, g_deriv)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation step (for 1 layer)\n",
    "def back_prop(dA, cache, activ_func):\n",
    "    m = dA.shape[1]\n",
    "    \n",
    "    A_prev, W, b, Z = cache\n",
    "    \n",
    "    if activ_func == 'relu':\n",
    "        dZ = relu_backward(dA, Z)\n",
    "    elif activ_func == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "\n",
    "    dW = 1/m * np.dot(dZ, A_prev.T)\n",
    "    db = 1/m * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backpropagation - full iteration (for L to 1 layers)\n",
    "def back_prop_model (AL, Y, caches):    # for whole NN (all layers) \n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Backpropagating over the last layer (layer L) with Sigmoid activaition function\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))        \n",
    "    current_cache = caches[L-1]    \n",
    "    grads['dW'+str(L)], grads['db'+str(L)], grads['dA'+str(L-1)] = back_prop(dAL, current_cache, 'sigmoid')    \n",
    "    \n",
    "    # Backpropagating over the layers from L-1 to 1 with ReLU activaition function\n",
    "    for l in reversed(range(L-1)):  # from l=L-2 to l=0\n",
    "        current_cache = caches[l]\n",
    "        dA = grads['dA'+str(l+1)]\n",
    "        grads['dW'+str(l+1)], grads['db'+str(l+1)], grads['dA'+str(l)] = back_prop(dA, current_cache, 'relu')\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating parameters W and b\n",
    "def update_params(params, grads, learning_rate):\n",
    "    L = len(params)//2\n",
    "    for l in range(L):\n",
    "        params['W'+str(l+1)] = params['W'+str(l+1)] - learning_rate * grads['dW'+str(l+1)]\n",
    "        params['b'+str(l+1)] = params['b'+str(l+1)] - learning_rate * grads['db'+str(l+1)]\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L-layers NN model\n",
    "def model_L_Layers(X, Y, layer_dims, num_iter = 2500, learning_rate = 0.0075):\n",
    "\n",
    "    parameters = init_params(layer_dims)\n",
    "    \n",
    "    for i in range(0, num_iter):\n",
    "        # Forward propagation\n",
    "        AL, caches = forward_prop_model (X, parameters)\n",
    "                \n",
    "        #Cost-function\n",
    "        cost = cost_func(AL, Y)\n",
    "        \n",
    "        # Backpropagation\n",
    "        grads = back_prop_model (AL, Y, caches)\n",
    "        \n",
    "        # Updating parameters W and b\n",
    "        parameters = update_params(parameters, grads, learning_rate)     \n",
    "\n",
    "        # print Cost-function on every 100-th iteration\n",
    "        if i%100 == 0:    \n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))    \n",
    "            \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model. As a result - get its parameters W and b.\n",
    "\n",
    "# Set the width and depth of the model \n",
    "n_x = train_set_x.shape[0]   #  input layer (layer 0)\n",
    "layer_dims = [n_x, 20, 7, 5, 1] #  4-layer model\n",
    "\n",
    "parameters = model_L_Layers(train_set_x, train_set_y, layer_dims, num_iter = 3000, learning_rate = 0.0075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Y, having W and b parameters\n",
    "def predict_Y (X, params):\n",
    "    m = X.shape[1]\n",
    "    Y_predict = np.zeros((1, m))\n",
    "\n",
    "    AL, caches = forward_prop_model(X, params)\n",
    "\n",
    "    for i in range(m):\n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]    \n",
    "        if AL[0,i] > 0.5:\n",
    "            Y_predict[0,i] = 1\n",
    "    return Y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Y on train and test datasets\n",
    "Y_train_prediction = predict_Y (train_set_x, parameters)\n",
    "Y_test_prediction = predict_Y (test_set_x, parameters)\n",
    "    \n",
    "# Print train and test accuracy\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_train_prediction - train_set_y)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_test_prediction - test_set_y)) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
